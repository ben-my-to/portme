<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Matplotlib on Jason Duong</title>
    <link>https://michaelneuper.github.io/hugo-texify3/tags/matplotlib/</link>
    <description>Recent content in Matplotlib on Jason Duong</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>my.toe.ben@gmail.com (Jason Duong)</managingEditor>
    <webMaster>my.toe.ben@gmail.com (Jason Duong)</webMaster>
    <lastBuildDate>Tue, 16 May 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://michaelneuper.github.io/hugo-texify3/tags/matplotlib/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A Parallel Decision Tree Classifier</title>
      <link>https://michaelneuper.github.io/hugo-texify3/posts/mpitree-project/</link>
      <pubDate>Tue, 16 May 2023 00:00:00 +0000</pubDate>
      <author>my.toe.ben@gmail.com (Jason Duong)</author>
      <guid>https://michaelneuper.github.io/hugo-texify3/posts/mpitree-project/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/ben-my-to/mpitree&#34;&gt;Source Code&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;try-it-out1&#34;&gt;Try it Out!&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://jsfiddle.net/nbkLudma/1/&#34;&gt;jsfiddle&lt;/a&gt;&lt;/p&gt;
&lt;!-- &lt;script type=&#34;text/javascript&#34; src=&#34;https://code.jquery.com/jquery-1.7.1.min.js&#34;&gt;&lt;/script&gt;

&lt;input type=&#34;range&#34; id=&#34;range&#34; min=&#34;0&#34; max=&#34;10&#34; value=&#34;10&#34; oninput=&#34;update_max_depth(this.value);&#34;&gt;
max_depth = &lt;output id=&#34;slider1-value&#34;&gt;&lt;/output&gt;

&lt;input type=&#34;range&#34; id=&#34;range2&#34; min=&#34;1&#34; max=&#34;50&#34; value=&#34;40&#34; oninput=&#34;update_n_samples(this.value);&#34;&gt;
n_samples = &lt;output id=&#34;slider2-value&#34;&gt;&lt;/output&gt;

&lt;input type=&#34;range&#34; id=&#34;range3&#34; min=&#34;1&#34; max=&#34;4&#34; value=&#34;3&#34; oninput=&#34;update_n_classes(this.value);&#34;&gt;
n_classes = &lt;output id=&#34;slider3-value&#34;&gt;&lt;/output&gt;

$\text{Train/Test Accuracy}$ = &lt;output id=&#34;accuracy&#34;&gt;&lt;/output&gt;

&lt;button type=&#34;button&#34; onclick=&#34;gen_points(); redraw()&#34; style=&#34;height: 40px; width: 200px;&#34;&gt;Generate New Training Data&lt;/button&gt;

&lt;figure&gt;
    &lt;canvas width=&#34;800&#34; height=&#34;400&#34; id=&#34;boundary&#34;&gt;&lt;/canvas&gt;&lt;br&gt;
    &lt;figurecaption&gt;Fig. 1: Decision Tree Classifier Demo&lt;/figurecaption&gt;
&lt;/figure&gt;

&lt;script type=&#34;text/javascript&#34;&gt;
// MODIFIED FROM: http://vision.stanford.edu/teaching/cs231n-demos/knn/knn.js

document.getElementById(&#34;slider1-value&#34;).innerHTML = document.getElementById(&#34;range&#34;).value;
document.getElementById(&#34;slider2-value&#34;).innerHTML = document.getElementById(&#34;range2&#34;).value;
document.getElementById(&#34;slider3-value&#34;).innerHTML = document.getElementById(&#34;range3&#34;).value;

Array.prototype.max = function () {
  return Math.max.apply(null, this);
};
Array.prototype.min = function () {
  return Math.min.apply(null, this);
};

const argFact = (compareFn) =&gt; (array) =&gt;
  array.map((el, idx) =&gt; [el, idx]).reduce(compareFn)[1];
const argMax = argFact((min, el) =&gt; (el[0] &gt; min[0] ? el : min));
const argMin = argFact((max, el) =&gt; (el[0] &lt; max[0] ? el : max));

function getCol(arr, n) {
  return arr.map((x) =&gt; x[n]);
}

function findMode(array) {
  let object = {};

  for (let i = 0; i &lt; array.length; i++) {
    if (object[array[i]]) {
      object[array[i]] += 1;
    } else {
      object[array[i]] = 1;
    }
  }

  let biggestValue = -1;
  let biggestValuesKey = -1;

  Object.keys(object).forEach((key) =&gt; {
    let value = object[key];
    if (value &gt; biggestValue) {
      biggestValue = value;
      biggestValuesKey = key;
    }
  });

  return biggestValuesKey;
}

var info_gains = [];
var thresholds = [];

class Node {
  constructor(value, threshold = null) {
    this.value = value;
    this.threshold = threshold;
    this.left = null;
    this.right = null;
  }

  is_leaf() {
    return this.left === null &amp;&amp; this.right === null;
  }
}

class DecisionTreeClassifier {
  constructor(max_depth = null) {
    this.tree = null;
    this.max_depth = max_depth;
  }

  unique(value, index, array) {
    return array.indexOf(value) === index;
  }

  entropy(x) {
    var counts = {};
    x.forEach(function (x) {
      counts[x] = (counts[x] || 0) + 1;
    });

    var n_unique_classes = [];
    for (var key in counts) {
      n_unique_classes.push(counts[key]);
    }

    var proba = [];
    for (var i = 0; i &lt; n_unique_classes.length; i++) {
      proba.push(n_unique_classes[i] / x.length);
    }

    var result = 0;
    for (var i = 0; i &lt; n_unique_classes.length; i++) {
      result += proba[i] * Math.log2(proba[i]);
    }

    return -result;
  }

  cond_entropy(X, y, t, feature_idx) {
    var X_left = X.filter((x) =&gt; x[feature_idx] &lt;= t);
    var X_right = X.filter((x) =&gt; x[feature_idx] &gt; t);

    var left_indices = [];
    var right_indices = [];
    for (var i = 0; i &lt; X.length; i++) {
      if (X[i][feature_idx] &lt;= t) left_indices.push(i);
      else right_indices.push(i);
    }

    var y_left = left_indices.map((x) =&gt; y[x]);
    var y_right = right_indices.map((x) =&gt; y[x]);

    var weight_left = X_left.length / X.length;
    var weight_right = X_right.length / X.length;

    var impurity_left = this.entropy(y_left);
    var impurity_right = this.entropy(y_right);

    return weight_left * impurity_left + weight_right * impurity_right;
  }

  information_gain(X, y, feature_idx) {
    var possible_thresholds = getCol(X, feature_idx).filter(this.unique);

    var costs = [];
    for (const t of possible_thresholds) {
      costs.push(this.cond_entropy(X, y, t, feature_idx));
    }

    return {
      gain: this.entropy(y) - costs.min(),
      threshold: possible_thresholds[argMin(costs)],
    };
  }

  fit(X, y) {
    this.n_features = X[0].length;
    this.tree = this.make_tree(X, y);
    return this;
  }

  make_tree(X, y, depth = 0) {
    var n_classes = y.filter(this.unique).length;

    if (
      n_classes == 1 ||
      (this.max_depth !== null &amp;&amp; this.max_depth == depth)
    ) {
      return new Node(findMode(y));
    }

    info_gains = [];
    thresholds = [];

    for (var i = 0; i &lt; this.n_features; i++) {
      var result = this.information_gain(X, y, i);
      info_gains.push(result[&#34;gain&#34;]);
      thresholds.push(result[&#34;threshold&#34;]);
    }

    var split_feature_idx = argMax(info_gains);
    var split_threshold = thresholds[split_feature_idx];

    var split_node = new Node(split_feature_idx, split_threshold);

    var left_indices = [];
    var right_indices = [];
    for (var i = 0; i &lt; X.length; i++) {
      if (X[i][split_feature_idx] &lt;= split_threshold) left_indices.push(i);
      else right_indices.push(i);
    }

    var X_left = X.filter((x) =&gt; x[split_feature_idx] &lt;= split_threshold);
    var X_right = X.filter((x) =&gt; x[split_feature_idx] &gt; split_threshold);

    split_node.left = this.make_tree(
      X_left,
      left_indices.map((x) =&gt; y[x]),
      depth + 1,
    );

    split_node.right = this.make_tree(
      X_right,
      right_indices.map((x) =&gt; y[x]),
      depth + 1,
    );

    return split_node;
  }

  predict(x) {
    var node = this.tree;
    while (!node.is_leaf()) {
      if (x[node.value] &lt;= node.threshold) {
        node = node.left;
      } else {
        node = node.right;
      }
    }
    return node.value;
  }
}

var RADIUS = 6;
var WIDTH = 800;
var HEIGHT = 400;
var canvas = document.getElementById(&#34;boundary&#34;);
canvas.width = WIDTH;
canvas.height = HEIGHT;

var ctx = canvas.getContext(&#34;2d&#34;);
ctx.height = HEIGHT;
ctx.width = WIDTH;

var state = {
  num_classes: 3,
  num_points: 40,
  max_depth: null,
  cluster_std: 50,
  colors: [&#34;red&#34;, &#34;blue&#34;, &#34;green&#34;, &#34;orange&#34;],
  small_step: 3,
  big_step: 10,
};

function update_max_depth(depth) {
  var slider_name = document.getElementById(&#34;slider1-value&#34;);
  slider_name.innerHTML = depth;
  state.max_depth = depth;
  redraw();
}

function update_n_samples(n_samples) {
  var slider2_name = document.getElementById(&#34;slider2-value&#34;);
  slider2_name.innerHTML = n_samples;
  state.num_points = n_samples;
  gen_points();
  redraw();
}

function update_n_classes(n_classes) {
  var slider3_name = document.getElementById(&#34;slider3-value&#34;);
  slider3_name.innerHTML = n_classes;
  state.num_classes = n_classes;
  gen_points();
  redraw();
}

function gen_points() {
  state.points = generate_cluster_points(
    ctx,
    state.num_classes,
    state.num_points,
    state.cluster_std,
  );
}
gen_points();

function redraw(speed) {
  var step = state.small_step;
  if (speed === &#34;fast&#34;) step = state.big_step;
  ctx.clearRect(0, 0, ctx.width, ctx.height);
  draw_boundaries(ctx, state, step);
  draw_points(ctx, state.points, state.colors);
}
redraw();

var dragging_point = null;
$(canvas).mousedown(function (e) {
  var p = get_click_coords(canvas, e);
  var thresh = 10;
  var idx = null;
  var min_dist = 100000;
  for (var i = 0; i &lt; state.num_points; i++) {
    var dx = p[0] - state.points[i][0];
    var dy = p[1] - state.points[i][1];
    var d = Math.sqrt(dx * dx + dy * dy);
    if (d &lt; thresh &amp;&amp; d &lt; min_dist) {
      min_dist = d;
      idx = i;
    }
  }
  dragging_point = idx;
});

$(canvas).mousemove(function (e) {
  if (dragging_point === null) return;
  var p = get_click_coords(canvas, e);
  state.points[dragging_point][0] = p[0];
  state.points[dragging_point][1] = p[1];
  redraw(&#34;fast&#34;);
});

$(canvas).mouseup(function () {
  dragging_point = null;
  redraw();
});

function get_click_coords(obj, e) {
  var offset = $(obj).offset();
  var cx =
    e.clientX +
    document.body.scrollLeft +
    document.documentElement.scrollLeft -
    Math.floor(offset.left);
  var cy =
    e.clientY +
    document.body.scrollTop +
    document.documentElement.scrollTop -
    Math.floor(offset.top) +
    1;
  return [cx, cy];
}

function randn() {
  // Using Box-Muller transform
  var u = 1 - Math.random();
  var v = 1 - Math.random();
  var r = Math.sqrt(-2 * Math.log(u));
  var t = Math.cos(2 * Math.PI * v);
  return r * t;
}

function dist(x1, y1, x2, y2) {
  return Math.sqrt(Math.pow(x1 - x2, 2) + Math.pow(y1 - y2, 2));
}

function collision(x1, y1, x2, y2) {
  eps = 2.5;
  return dist(x1, y1, x2, y2) &lt;= 2 * RADIUS + eps;
}

function generate_cluster_points(ctx, num_classes, num_points, std) {
  // First generate random cluster centers
  var centers = [];
  for (var c = 0; c &lt; num_classes; c++) {
    var x = ctx.width * Math.random();
    var y = ctx.height * Math.random();
    centers.push([x, y]);
  }

  let xmin = 25,
    xmax = 775;
  let ymin = 25,
    ymax = 375;

  // Now generate points near cluster centers
  var points = [];
  state.test = [];

  for (var i = 0; i &lt; num_points*2; i++) {
    var c = Math.floor(num_classes * Math.random());
    while (true) {
      var x = centers[c][0] + std * randn();
      var y = centers[c][1] + std * randn();

      x = Math.min(xmax, Math.max(xmin, x));
      y = Math.min(ymax, Math.max(ymin, y));

      var bad = false;
      for (const p of state.test) {
        if (collision(p[0], p[1], x, y)) bad = true;
      }

      if (!bad) break;
    }

    state.test.push([x, y, c]);
  }

  for (var i = 0; i &lt; num_points; i++) {
    var c = Math.floor(num_classes * Math.random());
    while (true) {
      var x = centers[c][0] + std * randn();
      var y = centers[c][1] + std * randn();

      x = Math.min(xmax, Math.max(xmin, x));
      y = Math.min(ymax, Math.max(ymin, y));

      var bad = false;
      for (const p of points) {
        if (collision(p[0], p[1], x, y)) bad = true;
      }

      if (!bad) break;
    }
    points.push([x, y, c]);
  }
  return points;
}

function draw_points(ctx, points, colors) {
  for (var i = 0; i &lt; points.length; i++) {
    var x = points[i][0];
    var y = points[i][1];
    var c = points[i][2];

    ctx.beginPath();
    ctx.globalAlpha = 1.0;
    ctx.fillStyle = colors[c];
    ctx.arc(x, y, RADIUS, 0, 2 * Math.PI);
    ctx.lineWidth = 2;
    ctx.stroke();
    ctx.fill();
  }
}

function fn(array, firstIndex, secondIndex) {
  return array.map((a) =&gt;
    a.filter((b, i) =&gt; i == firstIndex || i == secondIndex),
  );
}

function accuracy_score(clf, D) {
  var zip = rows =&gt; rows[0].map((_, c) =&gt; rows.map(row =&gt; row[c]));
  var n_correct = 0;

  X = fn(D, 0, 1);
  y = getCol(D, 2);

  X.forEach((x, idx) =&gt; {
    if (clf.predict(x) == y[idx])
      n_correct++;
  });

  return (n_correct / D.length).toFixed(2);
}

function draw_boundaries(ctx, state, step) {
  var eps = 0;
  var clf = new DecisionTreeClassifier(state.max_depth).fit(
    fn(state.points, 0, 1),
    getCol(state.points, 2),
  );

  for (var x = step / 2; x &lt; ctx.width; x += step) {
    for (var y = step / 2; y &lt; ctx.height; y += step) {
      var c = clf.predict([x, y]);

      if (c !== null) {
        ctx.globalAlpha = 0.4;
        ctx.fillStyle = state.colors[c];
        ctx.fillRect(
          x - step / 2 - eps,
          y - step / 2 - eps,
          step + 2 * eps,
          step + 2 * eps,
        );
      }
    }
  }
  document.getElementById(&#34;accuracy&#34;).innerHTML = accuracy_score(clf, state.points).toString() + &#34;/&#34; + accuracy_score(clf, state.test);
}
&lt;/script&gt; --&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;Decision Tree&lt;/strong&gt; is an $n$-nary tree where each node represents a feature &lt;em&gt;(interior nodes)&lt;/em&gt; or response &lt;em&gt;(terminal/leaf nodes)&lt;/em&gt; value, and each branch represents a condition on some feature. Decision Trees are intuitive &lt;em&gt;supervised&lt;/em&gt; machine learning algorithms for classification and regression problems. Decision Trees behave by posing questions about the data to narrow their choices until they are somewhat confident in their predictions. The fundamental procedure for a decision tree involves recursively querying each feature and partitioning the dataset and feature space into disjoint subsets and regions until there is no ambiguity about the response variable. The primary goal of any machine learning model is &lt;em&gt;generalization&lt;/em&gt; &amp;ndash; the model&amp;rsquo;s ability to perform well on future, unseen data. Therefore, the general approach to learning an optimal decision tree involves asking &amp;ldquo;good&amp;rdquo; questions &lt;em&gt;(&lt;strong&gt;split features&lt;/strong&gt; &amp;ndash; the features that maximize the information gain&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;)&lt;/em&gt; about the data that leads to the most certainty about the response variable each time.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;Decision Tree Classifier&lt;/strong&gt; is a decision tree whose prediction of a response variable is from a set of finite classes &lt;em&gt;(multi-classification)&lt;/em&gt;. Given some observed data, examples of classification-type problems are whether an incoming patient has cancer, whether an email is spam or not spam, or whether a fruit is an apple, banana, or orange.&lt;/p&gt;
&lt;h2 id=&#34;methodology&#34;&gt;Methodology&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;Parallel Decision Tree&lt;/strong&gt; algorithm exploits &lt;a href=&#34;https://en.wikipedia.org/wiki/Data_parallelism&#34;&gt;&lt;em&gt;data parallelism&lt;/em&gt;&lt;/a&gt; and aims to reduce the time taken by a &lt;em&gt;greedy&lt;/em&gt;  search across all features. It schedules processes to a number of sub-communicators in a &lt;em&gt;cyclic distribution&lt;/em&gt;&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;, roughly evenly across levels of a split feature. Processes in each sub-communicator concurrently participate in calculating the split feature and await completion at their parent communicator for all other processes in that communicator. Let $k$ be the total number of processes and $n$ be the number of levels, where $k,n\in\mathbb{N}$ such that $k\ge n\ge 2$. Then, a sub-communicator $m$ contains at most $\lceil k/n \rceil$ processes, and at least $[1\ldots n)$ processes. Each process&amp;rsquo;s identifier $p_{i\in[k]}$ is then assigned to the sub-communicator $m = i\bmod n$ and receives a unique identifier in that group $p_i = \lfloor i/n \rfloor$.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
  &lt;img src=&#34;https://raw.githubusercontent.com/ben-my-to/website/main/static/images/cyclic_distribution.png&#34; alt=&#34;Cyclic Distribution Example&#34; style=&#34;width:40%;display:block;margin-left:auto;margin-right:auto;&#34;&gt;
  &lt;figcaption&gt;Fig. 2: Cyclic Distribution&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;mathematical-modelling&#34;&gt;Mathematical Modelling&lt;/h3&gt;
&lt;div class=&#34;definition&#34;&gt;
&lt;p&gt;&lt;strong&gt;Definition 1&lt;/strong&gt;: A &lt;em&gt;cyclic distribution&lt;/em&gt; function $f_m:\left[k\right]\to\mathbf M$ takes as input a $k$-tuple of processes and outputs a set of communicators $\mathbf M$ is defined as&lt;/p&gt;
&lt;p&gt;$$
f_m(p_0,\ldots, p_{k-1}) = \lbrace m \rbrace\cup
\begin{cases}
\emptyset &amp;amp; k=1 \\
\bigcup_{j\in\left[n\right]}f_{mn+j+1}(p_i\mapsto\lfloor i/n \rfloor : i\bmod n=j) &amp;amp; \text{otherwise}.
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;for some initial natural numbers $k\ge n\ge 2$ and $m$.&lt;/p&gt;
&lt;/div &gt;
&lt;h2 id=&#34;model-evaluation-and-analysis&#34;&gt;Model Evaluation and Analysis&lt;/h2&gt;
&lt;h3 id=&#34;decision-boundaries-varying-values-for-the-max_depth-hyperparameter&#34;&gt;Decision Boundaries varying values for the &lt;code&gt;max_depth&lt;/code&gt; hyperparameter&lt;/h3&gt;
&lt;p&gt;As a decision tree grows deeper, overfitting becomes evident because predictions rely on increasingly smaller regions of the feature space. In a way, the decision tree model tends to bias toward &lt;em&gt;singleton nodes&lt;/em&gt;, potentially resulting in mispredictions in the presence of noisy data&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;Pre-and-post-pruning techniques are some solutions to reduce the likelihood of an overfitted decision tree. Pre-pruning techniques introduce early stopping criteria (e.g., &lt;code&gt;max_depth&lt;/code&gt;, &lt;code&gt;min_samples_split&lt;/code&gt; hyperparameters). In conjunction, validation methodologies such as $k$-fold Cross-Validation can be applied select optimal values for such hyperparameters.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/ben-my-to/website/main/static/images/iris_decision_tree.png&#34; alt=&#34;Decision Boundary Example&#34; style=&#34;width:70%;display:block;margin-left:auto;margin-right:auto;&#34;&gt;
  &lt;figcaption&gt;Fig. 3: Decision Boundaries on the Iris Dataset&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;parallel-execution-times&#34;&gt;Parallel Execution Times&lt;/h3&gt;
&lt;p&gt;Figure 4 below shows a plot of the number of training samples versus the time taken &lt;em&gt;(ms)&lt;/em&gt; to train four decision tree models &lt;em&gt;(one sequential and three in-parallel)&lt;/em&gt;. All decision tree models were trained with the same examples, sampled over a uniform distribution, across all iterations. Our experiment yields an average speedup of $\bar{S}=1.7$.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/ben-my-to/website/main/static/images/parallel_exec_time.png&#34; alt=&#34;Parallel Execution Time&#34; style=&#34;width:45%;display:block;margin-left:auto;margin-right:auto;&#34;&gt;
  &lt;figcaption&gt;Fig. 4: Parallel Execution Times&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;message-complexity&#34;&gt;Message Complexity&lt;/h3&gt;
&lt;div class=&#34;theorem&#34;&gt;
&lt;p&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt;: Given $n,k\in\mathbb{N}$ such that $k\ge n\ge 2$, the &lt;em&gt;Parallel Decision Tree&lt;/em&gt; classifier $\mathcal{T}$ exchanges at most $O(k^2)$ messages.&lt;/p&gt;
&lt;/div &gt;
&lt;p&gt;Proof by Induction.&lt;/p&gt;
&lt;p&gt;At every terminated recursive call from a parent communicator $|m|\ge 2$, each process must exchange $k$ messages for a total of $k^2$ messages through the &lt;code&gt;MPI.allgather&lt;/code&gt; function. As such, each process obtains a sub-tree computed by every other process.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/ben-my-to/website/main/static/images/message_complexity.png&#34; alt=&#34;Message Exchanges&#34; style=&#34;width:55%;display:block;margin-left:auto;margin-right:auto;&#34;&gt;
  &lt;figcaption&gt;Fig. 5: Communication Costs for Binary Trees&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div class=&#34;remark&#34;&gt;
&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;: We will use &lt;em&gt;integer division&lt;/em&gt; throughout the proof.&lt;/p&gt;
&lt;/div &gt;
&lt;p&gt;&lt;strong&gt;Definition 1&lt;/strong&gt; constructs a full $n$-nary communicator tree $\mathbf{M}$  &lt;em&gt;(each node has 0 or $n$ children)&lt;/em&gt; of height $h=\lceil\log_n k\rceil + 1$. Let $c\in\lbrace 0,\ldots,h-2\rbrace$ be a split performed by &lt;code&gt;MPI.Split&lt;/code&gt;. At split $c=0$, processors exchange $k^2$ messages. At the successor split $c=1$, processors exchange $n\cdot(k/n)^2$ messages. At split $c=3$, processors exchange $n^2\cdot(k/n^4)^2$ and so on and so forth&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;. Therefore, we can define the recurrence relation $\mathcal{M}(k)$, the total number of exchanged messages by $k$ processors, as&lt;/p&gt;
&lt;p&gt;$$
\mathcal{M}(k)=
\begin{cases}
0 &amp;amp; k=1\\
n^c\cdot k^2+\mathcal{M}(k/n) &amp;amp; \text{otherwise}.
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;Solving the recurrence relation, we obtain the general formula&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\mathcal{M}(k) &amp;amp;= n^{(0)} \cdot k^2 + \mathcal{M}(k/n)\\
&amp;amp;= (1)\cdot k^2 + \left[n^{(1)} \cdot (k/n)^2 + \mathcal{M}(k/n^2)\right]\\
&amp;amp;= {\color{red}k^2} + nk^2/n^2 + \left[n^{(2)} \cdot (k/n^2)^2 + \mathcal{M}(k/n^3)\right]\\
&amp;amp;= k^2 + {\color{red}k^2/n} + n^2 k^2/n^4 + \mathcal{M}(k/n^3) + \dots + \mathcal{M}(k/n^{h-2})\\
&amp;amp;= k^2 + k^2/n + {\color{red}k^2/n^2} + n^3 k^2/n^6 + \dots + \left[n^{\lceil\log_n(k)\rceil-1}\cdot (k/n^{\lceil\log_n(k)\rceil-1})^2 + \mathcal{M}(1)\right]\\
&amp;amp;= k^2 + k^2/n + k^2/n^2 + {\color{red}k^2/n^3} + \dots + (k/n)\cdot k^2/n^{2(\lceil\log_n(k)\rceil)-2} + 0\\
&amp;amp;= k^2 + k^2/n + k^2/n^2 + k^2/n^3 + \dots + (k/n)\cdot k^2n^2/k^2\\
&amp;amp;= k^2 + k^2/n + k^2/n^2 + k^2/n^3 + \dots + {\color{red}kn}.
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Thus, the worst-case message complexity of $\mathcal{T}$ is&lt;/p&gt;
&lt;p&gt;$$
O\left(\mathcal{M}(k)\right)=O\left(\sum_{c=0}^{h-2} k^2n^{-c}\right) = O\left(k^2+k^2n^{-1}+k^2 n^{-2}+\dots+kn\right) = O(k^2).
$$&lt;/p&gt;
&lt;p style=&#34;float:right&#34;&gt;$\blacksquare$&lt;/p&gt;
&lt;h2 id=&#34;future-works&#34;&gt;Future Works&lt;/h2&gt;
&lt;p&gt;Our implementation is restricted to binary trees ($n=2$) with numerical data. We plan to extend the functionality to handle categorical, sparse, missing, etc. data and regression problems in the next iteration.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Figure 1 provides an online visualization of the decision boundaries decided by a Decision Tree Classifier. Feel free to drag individual data points and sliders to explore how the decision boundary and accuracy of the model changes. This script is a modified version from &lt;a href=&#34;http://vision.stanford.edu/teaching/cs231n-demos/knn/&#34;&gt;CS231n-demos&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Information gain quantifies &lt;em&gt;(bits)&lt;/em&gt; the increase in confidence about a response variable after querying a feature. A higher value for information gain implies a greater likelihood of achieving purer splits &lt;em&gt;(no uncertainty on a prediction)&lt;/em&gt;.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;Figure 2 demonstrates the partitioning of communicator $m_0$ where the number of levels $n=2$ and number of processes $k=8$. Processes $p_0,p_2,p_4,p_6$ are scheduled to sub-communicator $m_1$ as each processor&amp;rsquo;s identifier is even &lt;em&gt;(divisible by 2)&lt;/em&gt;, and processes $p_1,p_3,p_5,p_7$ are scheduled to sub-communicator $m_2$ as each processor&amp;rsquo;s identifier is odd. Sub-communicator $m_i$ represent left subtrees of even process identifiers, and sub-communicator $m_j$ represents right subtrees of odd process identifiers.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;Figure 3 illustrates decision boundaries for different values of the &lt;code&gt;max_depth&lt;/code&gt; hyperparameter on the iris dataset provided by &lt;em&gt;scikit-learn&lt;/em&gt;. The figure showcases how noisy instances may negatively impact the performance of the decision tree classifier as the depth increases.&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;Figure 5 shows the number of messages exchanged at each level of a perfect &lt;em&gt;(although not necessary)&lt;/em&gt; binary tree. Each split $c$ reduces the number of processes by two until all remaining nodes are singletons, when $c=h-1$.&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
